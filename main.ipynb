{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "DATASET_COLUMNS = [\"target\", \"id\", \"time\", \"query\", \"to\", \"posts\"]\n",
    "data=pd.read_csv('data.csv', names=DATASET_COLUMNS , encoding='ISO-8859-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Analysis\n",
    "print(data[\"query\"].unique())\n",
    "data['target'].unique()\n",
    "data.dtypes\n",
    "data['target'] = data['target'].replace(4,1)\n",
    "data['target'].unique()\n",
    "#data['target'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping data according to target\n",
    "target_grouped =  data.groupby('target')['target'].count\n",
    "target_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data to lists\n",
    "target_list = list(data['target'])\n",
    "text_list = list(data['posts'])\n",
    "target_list\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing data distribution according to target\n",
    "import seaborn as sns\n",
    "sns.countplot(x='target', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting positive and negative data\n",
    "data_pos=data[data['target']==1]\n",
    "data_pos\n",
    "data_neg=data[data['target']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing data size to 25%\n",
    "data_pos=data_pos.iloc[:int(200000)]\n",
    "data_neg=data_neg.iloc[:int(200000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations except @\n",
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "english_punctuations = english_punctuations.replace(\"@\", \"\")\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "data['posts']= data['posts'].apply(lambda x: cleaning_punctuations(x))\n",
    "data['posts'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing username\n",
    "import re\n",
    "pattern = '@\\S+'\n",
    "def remove_pattern(text, pattern):\n",
    "    r = re.findall(pattern,text)\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    return text\n",
    "\n",
    "data['posts'] = data['posts'].apply(lambda x: remove_pattern(x, pattern))\n",
    "data['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing url\n",
    "import re\n",
    "pattern_url = 'http\\S+'\n",
    "def remove_pattern(text, pattern):\n",
    "    r = re.findall(pattern,text)\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    return text\n",
    "\n",
    "data['posts'] = data['posts'].apply(lambda x: remove_pattern(x, pattern_url))\n",
    "data['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing numbers\n",
    "pattern_num = '[0-9]'\n",
    "data['posts'] = data['posts'].apply(lambda x: remove_pattern(x, pattern_num))\n",
    "data['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [Awww, thats, a, bummer, You, shoulda, got, Da...\n",
       "1          [is, upset, that, he, cant, update, his, Faceb...\n",
       "2          [I, dived, many, times, for, the, ball, Manage...\n",
       "3          [my, whole, body, feels, itchy, and, like, its...\n",
       "4          [no, its, not, behaving, at, all, im, mad, why...\n",
       "                                 ...                        \n",
       "1599995    [Just, woke, up, Having, no, school, is, the, ...\n",
       "1599996    [TheWDBcom, Very, cool, to, hear, old, Walt, i...\n",
       "1599997    [Are, you, ready, for, your, MoJo, Makeover, A...\n",
       "1599998    [Happy, th, Birthday, to, my, boo, of, alll, t...\n",
       "1599999                              [happy, charitytuesday]\n",
       "Name: posts, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "\n",
    "data_posts_tokenized = data['posts'].apply(lambda x: x.split())\n",
    "data_posts_tokenized\n",
    "\n",
    "# https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4 @jatin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words, stemming we can skip, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(data.to_string())\n",
    "# data['posts'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove punctuations\n",
    "# import string\n",
    "# import re\n",
    "# punctuation = string.punctuation\n",
    "# punctuation = punctuation.replace(\"@\", \"\")\n",
    "# punctuation_list = list(punctuation)\n",
    "\n",
    "# \"\"\"\n",
    "# def remove_punctuation(text) :\n",
    "#     for i in punctuation_list :\n",
    "#         if (text.find(i)!=-1) :\n",
    "#             text.replace(i, \" \")\n",
    "#     return text\n",
    "# \"\"\"\n",
    "\n",
    "# def remove_punct(text):\n",
    "#     new_words = []\n",
    "#     for word in text:\n",
    "#         w = re.sub(r'[^\\w\\s\\@]','',word) #remove everything except words and space\n",
    "#         w = re.sub(r'_','',w) #how to remove underscore as well\n",
    "#         new_words.append(w)\n",
    "#     return ''.join(str(e) for e in new_words)\n",
    "    \n",
    "\n",
    "# data['posts'] = data['posts'].apply(lambda x : remove_punct(x))\n",
    "# data['posts']\n",
    "# new_data = data['posts']\n",
    "\n",
    "# #data['posts'] = data['posts'].str.replace(\"[^@a-zA-Z#]\", \" \")\n",
    "# #data['posts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data['posts'][0])\n",
    "\n",
    "#for i in range(80000,800000) :\n",
    "#    data['posts'][i]=remove_pattern(data['posts'][i], pattern)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
